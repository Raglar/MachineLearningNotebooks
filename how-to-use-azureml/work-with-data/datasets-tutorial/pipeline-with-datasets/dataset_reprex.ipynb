{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License [2017] Zalando SE, https://tech.zalando.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/MachineLearningNotebooks/how-to-use-azureml/work-with-data/datasets-tutorial/pipeline-with-datasets/pipeline-for-image-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a simple ML pipeline for image classification\n",
    "\n",
    "## Introduction\n",
    "This tutorial shows how to train a simple deep neural network using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset and Keras on Azure Machine Learning. Fashion-MNIST is a dataset of Zalando's article imagesâ€”consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes.\n",
    "\n",
    "Learn how to:\n",
    "\n",
    "> * Set up your development environment\n",
    "> * Create the Fashion MNIST dataset\n",
    "> * Create a machine learning pipeline to train a simple deep learning neural network on a remote cluster\n",
    "> * Retrieve input datasets from the experiment and register the output model with datasets\n",
    "\n",
    "## Prerequisite:\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](../../../configuration.ipynb) to:\n",
    "    * install the latest version of AzureML SDK\n",
    "    * create a workspace and its configuration file (`config.json`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Creating an experiment to track all your runs\n",
    "* Creating a remote compute target to use for training\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages you need in this session. Also display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.0.85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore, ComputeTarget, RunConfiguration, Experiment\n",
    "# from azureml.core import Dataset\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.opendatasets import Diabetes\n",
    "from azureml.core.runconfig import CondaDependencies\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `workspace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: avadevitsmlsvc\n",
      "Azure region: westus2\n",
      "Subscription id: ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e\n",
      "Resource group: RG-ITSMLTeam-Dev\n"
     ]
    }
   ],
   "source": [
    "# load workspace\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment and a directory\n",
    "\n",
    "Create an experiment to track the runs in your workspace and a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML experiment\n",
    "exp = Experiment(workspace=workspace, name='dataset-bug')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource\n",
    "By using Azure Machine Learning Compute, a managed service, data scientists can train machine learning models on clusters of Azure virtual machines. Examples include VMs with GPU support. In this tutorial, you create Azure Machine Learning Compute as your training environment. The code below creates the compute clusters for you if they don't already exist in your workspace.\n",
    "\n",
    "**Creation of compute takes approximately 5 minutes.** If the AmlCompute with that name is already in your workspace the code will skip the creation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target\n",
      "{'currentNodeCount': 1, 'targetNodeCount': 1, 'nodeStateCounts': {'preparingNodeCount': 0, 'runningNodeCount': 0, 'idleNodeCount': 1, 'unusableNodeCount': 0, 'leavingNodeCount': 0, 'preemptedNodeCount': 0}, 'allocationState': 'Steady', 'allocationStateTransitionTime': '2020-02-03T22:32:45.539000+00:00', 'errors': None, 'creationTime': '2020-01-30T19:57:41.349092+00:00', 'modifiedTime': '2020-01-30T21:40:23.377992+00:00', 'provisioningState': 'Succeeded', 'provisioningStateTransitionTime': None, 'scaleSettings': {'minNodeCount': 0, 'maxNodeCount': 4, 'nodeIdleTimeBeforeScaleDown': 'PT600S'}, 'vmPriority': 'Dedicated', 'vmSize': 'STANDARD_NC6'}\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "# cluster_name = \"your-cluster-name\" # TODO CHANGE THIS DEMO COMPUTE TO BE DIFFERENT\n",
    "cluster_name = \"dataset-bug\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=workspace, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"https://azureopendatastorage.blob.core.windows.net/mlsamples/diabetes/*.parquet\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetFiles\",\n",
       "    \"ReadParquetFile\",\n",
       "    \"DropColumns\"\n",
       "  ]\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_raw = Diabetes.get_tabular_dataset()\n",
    "\n",
    "dataset_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset at 0x1fc3ae79b38>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore = workspace.get_default_datastore()\n",
    "# define output data\n",
    "output_dir = PipelineData('output_dir', datastore=datastore).as_dataset()\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azureml.pipeline.core.pipeline_output_dataset.PipelineOutputFileDataset at 0x1fc31673e10>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will work\n",
    "output_dir = output_dir.register(name='output_dir', create_new_version=True)\n",
    "\n",
    "# # this does not work\n",
    "# output_dir = output_dir.parse_delimited_files()\n",
    "# output_dir = output_dir.register(name='output_dir', create_new_version=True)\n",
    "\n",
    "output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build 2-step ML pipeline\n",
    "\n",
    "The [Azure Machine Learning Pipeline](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines) enables data scientists to create and manage multiple simple and complex workflows concurrently. A typical pipeline would have multiple tasks to prepare data, train, deploy and evaluate models. Individual steps in the pipeline can make use of diverse compute options (for example: CPU for data preparation and GPU for training) and languages. [Learn More](https://github.com/Azure/MachineLearningNotebooks/tree/master/how-to-use-azureml/machine-learning-pipelines)\n",
    "\n",
    "\n",
    "### Step 1: data preparation\n",
    "\n",
    "In step one, we will load the image and labels from Fashion MNIST dataset into mnist_train.csv and mnist_test.csv\n",
    "\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. Both mnist_train.csv and mnist_test.csv contain 785 columns. The first column consists of the class labels, which represent the article of clothing. The rest of the columns contain the pixel-values of the associated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda = CondaDependencies(conda_dependencies_file_path='compute_dependencies_small.yml')\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.python.conda_dependencies = conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate data (or output of a step) is represented by a `PipelineData` object. preprared_fashion_ds is produced as the output of step 1, and used as the input of step 2. PipelineData introduces a data dependency between steps, and creates an implicit execution order in the pipeline. You can register a `PipelineData` as a dataset and version the output data automatically. [Learn More](https://docs.microsoft.com/azure/machine-learning/service/how-to-version-track-datasets#version-a-pipeline-output-dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **PythonScriptStep** is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a [**RunConfiguration**](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfiguration?view=azure-ml-py) to specify requirements for the PythonScriptStep, such as conda dependencies and docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_step = PythonScriptStep(\n",
    "    name='extract',\n",
    "    script_name='extract.py',\n",
    "    arguments=['--output_dir', output_dir,\n",
    "               '--remote_run', True,\n",
    "               #    '--n_rows', 10000\n",
    "               ],\n",
    "    compute_target=compute_target,\n",
    "    inputs=[dataset_raw.as_named_input('is_there_under__score_limit')],\n",
    "    outputs=[output_dir],\n",
    "    runconfig=run_config,\n",
    "    source_directory=os.path.join(os.getcwd(), 'extract'),\n",
    "    allow_reuse=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: train CNN with Keras\n",
    "\n",
    "Next, we construct an `azureml.train.dnn.TensorFlow` estimator object. The TensorFlow estimator is providing a simple way of launching a TensorFlow training job on a compute target. It will automatically provide a docker image that has TensorFlow installed.\n",
    "\n",
    "[EstimatorStep](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps.estimator_step.estimatorstep?view=azure-ml-py) adds a step to run Tensorflow Estimator in a Pipeline. It takes a dataset as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # set up training step with Tensorflow estimator\n",
    "# est = TensorFlow(entry_script='train.py',\n",
    "#                  source_directory=script_folder,                 \n",
    "#                  pip_packages = ['azureml-sdk','keras','numpy','scikit-learn', 'matplotlib'],\n",
    "#                  compute_target=compute_target)\n",
    "\n",
    "# est_step = EstimatorStep(name='train step',\n",
    "#                          estimator=est,\n",
    "#                          estimator_entry_script_arguments=[],\n",
    "#                          # parse prepared_fashion_ds into TabularDataset and use it as the input\n",
    "#                          inputs=[prepared_fashion_ds.parse_delimited_files()],\n",
    "#                          compute_target=compute_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py).\n",
    "\n",
    "A pipeline is created with a list of steps and a workspace. Submit a pipeline using [submit](https://docs.microsoft.com/python/api/azureml-core/azureml.core.experiment(class)?view=azure-ml-py#submit-config--tags-none----kwargs-). When submit is called, a [PipelineRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.pipelinerun?view=azure-ml-py) is created which in turn creates [StepRun](https://docs.microsoft.com/python/api/azureml-pipeline-core/azureml.pipeline.core.steprun?view=azure-ml-py) objects for each step in the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline & run experiment\n",
    "pipeline = Pipeline(workspace,\n",
    "                    steps=[\n",
    "                        extract_step,\n",
    "#                         est_step\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step extract [390c8830][7a162250-cd67-44be-8ceb-823353e0b52b], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 262d8286-3074-477b-8bb0-3a0d69d49869\n",
      "Link to Azure Machine Learning studio: https://ml.azure.com/experiments/dataset-bug/runs/262d8286-3074-477b-8bb0-3a0d69d49869?wsid=/subscriptions/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e/resourcegroups/RG-ITSMLTeam-Dev/workspaces/avadevitsmlsvc\n"
     ]
    }
   ],
   "source": [
    "run = exp.submit(pipeline, regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.tag(\"Dataset\",\"Diabetes\")\n",
    "run.tag(\"pipeline\", \"reprex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the PipelineRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 262d8286-3074-477b-8bb0-3a0d69d49869\n",
      "Link to Portal: https://ml.azure.com/experiments/dataset-bug/runs/262d8286-3074-477b-8bb0-3a0d69d49869?wsid=/subscriptions/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e/resourcegroups/RG-ITSMLTeam-Dev/workspaces/avadevitsmlsvc\n",
      "PipelineRun Status: NotStarted\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: 57fa46a4-c900-4642-a7ce-54d70705e6c5\n",
      "Link to Portal: https://ml.azure.com/experiments/dataset-bug/runs/57fa46a4-c900-4642-a7ce-54d70705e6c5?wsid=/subscriptions/ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e/resourcegroups/RG-ITSMLTeam-Dev/workspaces/avadevitsmlsvc\n",
      "StepRun( extract ) Status: NotStarted\n",
      "StepRun( extract ) Status: Running\n",
      "\n",
      "Streaming azureml-logs/55_azureml-execution-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt\n",
      "========================================================================================================================\n",
      "2020-02-03T22:46:31Z Starting output-watcher...\n",
      "Login Succeeded\n",
      "Using default tag: latest\n",
      "latest: Pulling from azureml/azureml_05e7bbe9beafec2417a9b085e6a0a2a8\n",
      "Digest: sha256:62ae199324fecdf8358ba9fe9698ea6d776d0247324ed4abdb5b17d9ee67895f\n",
      "Status: Image is up to date for avadevitsmlsvc4326118371.azurecr.io/azureml/azureml_05e7bbe9beafec2417a9b085e6a0a2a8:latest\n",
      "c99a2f3a67fa4773d03826dd85fbdd6cf2167c49280525414849824e9a58e1e6\n",
      "2020/02/03 22:46:33 Version: 3.0.01110.0001 Branch: master Commit: ab88b58d\n",
      "2020/02/03 22:46:34 /dev/infiniband/uverbs0 found (implying presence of InfiniBand)?: false\n",
      "2020/02/03 22:46:34 sshd runtime has already been installed in the container\n",
      "ssh-keygen: /azureml-envs/azureml_ef603ddd7729973f7484ef984f5589d2/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "ssh-keygen: /azureml-envs/azureml_ef603ddd7729973f7484ef984f5589d2/lib/libcrypto.so.1.0.0: no version information available (required by ssh-keygen)\n",
      "\n",
      "Streaming azureml-logs/65_job_prep-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt\n",
      "===============================================================================================================\n",
      "Starting job preparation. Current time:2020-02-03T22:46:42.951055\n",
      "Extracting the control code.\n",
      "Creating directory: azureml-logs/\n",
      "Retrieving project from snapshot: 3609eee3-fa44-4654-b566-58b573dec850\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 99\n",
      "\n",
      "Streaming azureml-logs/70_driver_log.txt\n",
      "========================================\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 153\n",
      "Entering Run History Context Manager.\n",
      "all args:\n",
      "{'input_dir': 'C:\\\\\\\\temp',\n",
      " 'n_rows': None,\n",
      " 'output_dir': '/mnt/batch/tasks/shared/LS_root/jobs/avadevitsmlsvc/azureml/57fa46a4-c900-4642-a7ce-54d70705e6c5/mounts/workspaceblobstore/azureml/57fa46a4-c900-4642-a7ce-54d70705e6c5/output_dir',\n",
      " 'remote_run': True}\n",
      "Azure ML SDK Version:  1.0.85\n",
      "Azure ML dataprep Version:  1.1.38\n",
      "is this none? nrows =  <class 'NoneType'>\n",
      "input_datasets:\n",
      " []\n",
      "Using None dataset version None\n",
      "start_time 2020-02-03 22:46:52\n",
      "__end_time 2020-02-03 22:46:57\n",
      "2020-02-03 22:46:57 4.78 secs to read None rows\n",
      "2020-02-03 22:46:57 initial shape: (442, 11)\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Cleaning up all outstanding Run operations, waiting 300.0 seconds\n",
      "2 items cleaning up...\n",
      "Cleanup took 0.0012936592102050781 seconds\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 153\n",
      "\n",
      "Streaming azureml-logs/75_job_post-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt\n",
      "===============================================================================================================\n",
      "Starting job release. Current time:2020-02-03T22:47:02.805978\n",
      "Logging experiment finalizing status in history service.\n",
      "Starting the daemon thread to refresh tokens in background for process with pid = 236\n",
      "Job release is complete. Current time:2020-02-03T22:47:04.608536\n",
      "\n",
      "StepRun(extract) Execution Summary\n",
      "===================================\n",
      "StepRun( extract ) Status: Finished\n",
      "\n",
      "Warnings:\n",
      "This compute target type doesn't support non-Docker runs; overriding run configuration to enable Docker.\n",
      "Please enable Docker in the environment section of your run configuration to stop seeing this warning message.\n",
      "{'runId': '57fa46a4-c900-4642-a7ce-54d70705e6c5', 'target': 'dataset-bug', 'status': 'Completed', 'startTimeUtc': '2020-02-03T22:46:30.627533Z', 'endTimeUtc': '2020-02-03T22:47:20.782715Z', 'warnings': [{'message': \"This compute target type doesn't support non-Docker runs; overriding run configuration to enable Docker.\\nPlease enable Docker in the environment section of your run configuration to stop seeing this warning message.\"}], 'properties': {'azureml.runsource': 'azureml.StepRun', 'ContentSnapshotId': '3609eee3-fa44-4654-b566-58b573dec850', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.pipelinerunid': '262d8286-3074-477b-8bb0-3a0d69d49869', '_azureml.ComputeTargetType': 'amlcompute', 'AzureML.DerivedImageName': 'azureml/azureml_05e7bbe9beafec2417a9b085e6a0a2a8', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': '4742470a-7984-4850-9d92-8537926e9bf3'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'is_there_under__score_limit', 'mechanism': 'Direct'}}], 'runDefinition': {'script': 'extract.py', 'arguments': ['--output_dir', '$AZUREML_DATAREFERENCE_output_dir', '--remote_run', 'True'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'dataset-bug', 'dataReferences': {'output_dir': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/57fa46a4-c900-4642-a7ce-54d70705e6c5/output_dir', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'is_there_under__score_limit': {'dataLocation': {'dataset': {'id': '4742470a-7984-4850-9d92-8537926e9bf3'}, 'dataPath': None}, 'createOutputDirectories': False, 'mechanism': 'Direct', 'environmentVariableName': 'is_there_under__score_limit', 'pathOnCompute': None, 'overwrite': False}}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'environment': {'name': 'Experiment dataset-bug Environment', 'version': 'Autosave_2020-01-31T03:41:25Z_1f7e044d', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', 'pip=19.3.1', {'pip': ['azureml-defaults==1.0.85', 'azureml-dataprep[fuse]==1.1.38', 'numpy==1.17.3', 'pandas==0.25.3', 'pyarrow==0.15.1', 'featuretools==0.12.0']}], 'name': 'azureml_ef603ddd7729973f7484ef984f5589d2'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04', 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'shmSize': '1g'}, 'spark': {'repositories': ['[]'], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs']}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/55_azureml-execution-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt?sv=2019-02-02&sr=b&sig=gK568RQzt%2B9%2FpwOvbFSx8kKBV551leOKXcHRnQ7eqhs%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'azureml-logs/65_job_prep-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/65_job_prep-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt?sv=2019-02-02&sr=b&sig=KV6RnNGc3JMl%2BPNz%2BwPfQrW6ah7VcqT3C%2FGhJ5CpQBI%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=mxAeeJ4Vmr7nTUnSNp3D00AeklZS3OMJ0cFch0Ayz5o%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'azureml-logs/75_job_post-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/75_job_post-tvmps_e4295d55c6843f4b794d878afc778c9a12e83d9e7817a3bf7c05af774f2403af_d.txt?sv=2019-02-02&sr=b&sig=KPshgQ%2FVqZVH1N07VlFbNcvZMfh5trjF%2FyYgf0UwRgA%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'azureml-logs/process_info.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=k8jOjon3PWb9s4gARtKpONI9dxvKDWjwoLZPVV3DPjM%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'azureml-logs/process_status.json': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=Ul%2B14%2FYJyx8NjPwz9L%2FN20tI12RFOx4Tqqeg4AMfhhM%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/153_azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/153_azureml.log?sv=2019-02-02&sr=b&sig=gOVi5%2Bs3vo7OCPYPNoSLmL%2Fe5DgUTZwIh7BqUVWFvRw%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=EibAGJdmSzQ%2FZs8bjLpHgI4MeRfobOcv5dK9MkZdD80%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=H%2F4G94766pMHlKgOOBD8yI5X%2Bx3Qv73FkIJgaGiAli8%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=UEDGRbH2DP%2BhWfnGDRQ6P%2F7bO1z3By8Yyh7QjoeXktE%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=szG%2BhWw0%2BH0mjikJePNq5POnLHfRyKU%2BlFORmK3Prw0%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.57fa46a4-c900-4642-a7ce-54d70705e6c5/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=I1FJzWb7zTIGIit032H90ZjuKeIAUK3xZKGwEeJt%2BwE%3D&st=2020-02-03T22%3A37%3A32Z&se=2020-02-04T06%3A47%3A32Z&sp=r'}}\n",
      "\n",
      "\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '262d8286-3074-477b-8bb0-3a0d69d49869', 'status': 'Completed', 'startTimeUtc': '2020-02-03T22:45:47.479249Z', 'endTimeUtc': '2020-02-03T22:47:30.970406Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.262d8286-3074-477b-8bb0-3a0d69d49869/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=Z9yYXgEVGc6xRD1xFWLQOtN8rSrmkvOybxt3Y%2FDFpwE%3D&st=2020-02-03T22%3A37%3A33Z&se=2020-02-04T06%3A47%3A33Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.262d8286-3074-477b-8bb0-3a0d69d49869/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=PadvAmTQG7vClFoUI%2BRZI%2F5TpzbfjtpNiRUQfm4od3A%3D&st=2020-02-03T22%3A37%3A33Z&se=2020-02-04T06%3A47%3A33Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://avadevitsmlsvc7139228118.blob.core.windows.net/azureml/ExperimentRun/dcid.262d8286-3074-477b-8bb0-3a0d69d49869/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=2kZxAy%2FEciORuhUdoerA4y1nkbHARUY%2BzS4%2BIdaA5Pk%3D&st=2020-02-03T22%3A37%3A33Z&se=2020-02-04T06%3A47%3A33Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"source\": [\n",
       "    \"('workspaceblobstore', 'azureml/57fa46a4-c900-4642-a7ce-54d70705e6c5/output_dir')\"\n",
       "  ],\n",
       "  \"definition\": [\n",
       "    \"GetDatastoreFiles\"\n",
       "  ],\n",
       "  \"registration\": {\n",
       "    \"id\": \"e848e50e-5220-4a0c-8a80-545b6fde84e2\",\n",
       "    \"name\": \"output_dir\",\n",
       "    \"version\": 9,\n",
       "    \"workspace\": \"Workspace.create(name='avadevitsmlsvc', subscription_id='ff2e23ae-7d7c-4cbd-99b8-116bb94dca6e', resource_group='RG-ITSMLTeam-Dev')\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_new = Dataset.get_by_name(workspace, 'output_dir')\n",
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "sihhu"
   }
  ],
  "category": "tutorial",
  "compute": [
   "Remote"
  ],
  "datasets": [
   "Fashion MNIST"
  ],
  "deployment": [
   "None"
  ],
  "exclude_from_index": false,
  "framework": [
   "Azure ML"
  ],
  "friendly_name": "Datasets with ML Pipeline",
  "index_order": 1,
  "kernelspec": {
   "display_name": "Python (mkt_aml85)",
   "language": "python",
   "name": "mkt_aml85"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "star_tag": [
   "featured"
  ],
  "tags": [
   "Dataset",
   "Pipeline",
   "Estimator",
   "ScriptRun"
  ],
  "task": "Train"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
